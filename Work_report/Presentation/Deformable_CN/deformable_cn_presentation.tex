\documentclass{beamer}
\usetheme{Boadilla}
% \usepackage[a4paper, tmargin=1in, bmargin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{pdflscape}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{multirow}
\mode<presentation>{
    \AtBeginSection[]
    {
    \begin{frame}[allowframebreaks]{Outline}
    \tableofcontents[currentsection]
    \end{frame}
    }
}
% \AtBeginSubsection[
%   {\frame<beamer>{\frametitle{Outline}
%     \tableofcontents[currentsection,currentsubsection]}}%
% ]%
% {
%   \frame<beamer>{
%     \frametitle{Outline}
%     \tableofcontents[currentsection,currentsubsection]}
% }

\title{Deformable Convolution Networks}
% \subtitle{Using Beamer}
\author{Arka Sadhu}
\institute{IIT Bombay}
\date{\today}


\begin{document}
% document goes here


\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Outline}
\tableofcontents
\end{frame}

\section{Deformable Convolutional Networks : Introduction}

\begin{frame}
  \frametitle{Limitations of Convolutional Networks}

  \begin{itemize}
  \item CNNs cannot model large unknown transformations because of fixed geometric structures of CNN modules.
  \item Convolution samples features at fixed locations.
  \item Region of Interest (RoI) use fixed spatial bins.
  \item Example : Receptive fields of a convolution layer is the same at all places. This is not desirable at higher layers which encode semantic features rather than spatial features.
  \item Instead of bounding boxes we would rather want exact boundaries.
  \item Hence we move on to Deformable Convolutional Networks.
  \end{itemize}
\end{frame}

% \section{Two Modules Introduced}
\begin{frame}
  \frametitle{Two New Modules}
  \begin{itemize}
  \item Deformable Convolutions : basic idea is to add 2d offset to enable a deformed sampling grid. These offset are also learnt simultaneously along with the convolutional layers.
  \item Deformable RoI : similar idea. Adds offset to each bin position in the regular bin partitioning.
  \item Combined to get Deformable Convolutional Networks.
  \item Authors claim that this can directly replace existing CNN architecture.
  \end{itemize}
\end{frame}

\section{Deformable Convolutions} %
% \subsection{Basics of Spectral Approach}
\begin{frame}
  \frametitle{Simple Convolution to Deformable Convolutions}
  \begin{itemize}
  % \item While the feature maps and convolutions in a CNN are in 3D, both the deformable convolution and deformable roi are in 2D.
  \item Let $R$ denote the set of points which are to be considered for the convolution. In usual convolution of size 3 this $R$ will have $(-1,-1)$ to $(1, 1)$.
  \item Let input feature map be denoted by x and output feature map denoted by y, and w be in the weights of the convolution filter. For a particular point $p_0$, $$y(p_0) = \sum_{p_n \in R}w(p_n) x(p_0 + p_n)$$.
  \item For the case of deformable convolutions the new equation will be
    $$y(p_0) = \sum_{p_n \in R}w(p_n) x(p_0 + p_n + \Delta p_n)$$.
  \end{itemize}
\end{frame}

% \subsection{Problem Formulation}
\begin{frame}
  \frametitle{Simple Convolution to Deformable Convolutions (Contd.)}
  \begin{itemize}
  \item Note: $\Delta p_n$ can be fractional. To get the value of $x(p_0 + p_n + \Delta p_n)$ bilinear interpolation is used.
  \item Let G(., .) be the bilinear interpolation kernet. Then for any point $p$ (could be fractional as well) $$x(p) = \sum_{q}G(p,q) x(q)$$
  \item Authors claim that this is easy to compute since G will be non-zero at very small number of qs.
  \end{itemize}
\end{frame}

% \subsection{Graph Laplacian}
\subsection{Deformable RoI and Deformable RoI pooling}
\begin{frame}
  \frametitle{What is RoI and RoI pooling}
  \begin{itemize}
  \item RoI is region of interest. The best example would be a bounding box for an object in an image.
  \item We would like to work even when this bounding box is not be constrained to rectangular.
  \item RoI pooling divides the RoI into k by k bins and outputs a feature map y of size k-by-k. This could be max or average pooling or any other kind of pooling. For say (i, j)-th bin with $n_{ij}$ pixels we can have:
    $$y(i, j) = \sum_{p \in bin(i,j)}x(p_0 + p)/n_{ij}$$
  % \item
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{RoI pooling to Deformable RoI pooling}
  \begin{itemize}
  \item For the deformable RoI pooling case we will instead have: $$y(i, j) = \sum_{p \in bin(i,j)}x(p_0 + p + \Delta p_{ij)})/n_{ij}$$
  \item Again $\Delta p_{ij}$ could be fractional and we would use bilinear interpolation.
  \item The paper introduces the idea of normalized offsets $\hat{\Delta{p_{ij}}}$ and actual offset is calculated using $\Delta p_{ij} = \gamma *  \hat{\Delta{p_{ij}}} \cdot(w, h)$. This is intuitively required to account for the different k used in the RoI pooling. Emperically $\gamma$ is set to 0.1
  \end{itemize}
\end{frame}


% \begin{frame}

%   \begin{itemize}
%   \item For the deformable RoI pooling case we will instead have:
%     $$y(i, j) = \sum_{p \in bin(i,j)}x(p_0 + p + \Delta p_{ij)/n_{ij}$$
%   \end{itemize}
% \end{frame}

% \section{Spectral Networks and Deep Locally Connected Networks on Graphs}
% \begin{frame}
%   \frametitle{Spectral Networks and Deep Locally Connected Networks on Graphs}
%   \begin{itemize}
%   \item Mentions the use of both spatial and spectral construction.
%   \item For the spectral part uses a spline and has k control points for it.
%     $$g_{\theta}(\Lambda) = B\theta$$
%     Here $B$ is the cubic B-spline basis and $\theta$ is a vector of control points.
%   \item The datasets used (created) are quite interesting. Subsampled MNIST and MNIST on sphere to show how spectral networks can be used on graphs.
%   \end{itemize}
% \end{frame}

% \section{CNN on Graphs with Fast Localized Spectral Filtering}
% \subsection{Learning fast localized Spectral filters}
% \begin{frame}
%   \frametitle{Graph Fourier Transform}
%   \begin{itemize}
%   \item Laplcian of the graph is real symmetric positive semidefinite, and thus can be written as
%     $$L = U \Lambda U^{T}$$
%   \item Here $U = [u_0 .... u_{n-1}]$ is the fourier basis and $\Lambda = diag([\lambda_0...\lambda_{n-1}])$ are ordered real non-negative eigen values.
%   \item Graph Fourier Transform of a signal $x$ is $\hat{x} = U^{T}x$.
%   \end{itemize}
% \end{frame}

% \begin{frame}
%   \frametitle{Spectral filtering of graph signals}
%   \begin{itemize}
%   \item Defining convolution on graphs
%     $$ x *_{G} y = U((U^T x) \odot (U^T y))$$
%   \item Filtering by $g_{\theta}$
%     $$ y = g_{\theta}(L)x = g_{\theta}(U \Lambda U^{T})x = Ug_{\theta}(\Lambda) U^T x $$
%   \item A non-parametric filter (all parameters free) would be defined as
%     $$g_{\theta}(\Lambda) = diag(\theta)$$
%   \end{itemize}
% \end{frame}

% \begin{frame}
%   \frametitle{Polynomial Parametrization}
%   \begin{itemize}
%   \item   Problem with non-parametric filters is that not localized (we want something like k-neighborhood) and therefore their learning complexity becomes $O(n)$. This can be overcomed with use of a Polynomial filter
%     $$g_{\theta}(\Lambda) = \sum_{k = 0}^{K-1}\theta_k\Lambda^k$$
%   \item The advantage we gain here is that nodes which are at a distance greater than $K$ away from the node $i$, at which the filter is applied, are not affected. Hence we have gained localization.
%   \end{itemize}
% \end{frame}

% \begin{frame}
%   \frametitle{Recursive formulation for fast filtering}
%   \begin{itemize}
%   \item Still cost to filter is high $O(n^2)$ because of multiplication with $U$ matrix.
%   \item Therefore use recurrence relation of chebyshev polynomial instead.
%     $$g_{\theta}(\Lambda) = \sum_{k=0}^{K-1}\theta_k T_K(\tilde{\Lambda})$$
%     Here $\tilde{\Lambda}$ is scaled between $[-1,1]$.
%   \item This allows us to compute $\bar{x_k} = T_K{\tilde{L}}x$. And Therefore
%     $$ y = g_{\theta}(L)x = [\bar{x_0}...\bar{x_{k-1}}]\theta$$
%   \item The cost is now $O(K|E|)$
%   \end{itemize}
% \end{frame}

% \begin{frame}
%   \frametitle{Learning filters}
%   \begin{itemize}
%   \item Trivial to show that backprop calculation can be done efficiently.
%   \end{itemize}
% \end{frame}

% \subsection{Coarsening and Pooling}
% \begin{frame}
%   \frametitle{Graph Coarsening and Pooling}
%   \begin{itemize}
%   \item Require efficient mechanism for pooling. Graph clustering as such is NP-hard and some approximations must be made.
%   \item The paper uses Graclus algorithm for coarsening, and uses an intelligent way of rearranging the nodes [creating a balanced binary tree from the remaining singleton and fake nodes] so that the pooling now becomes equivalent to pooling a regular 1D signal.
%   \end{itemize}
% \end{frame}

% % \subsection{Results on MNIST}
% \begin{frame}
%   \frametitle{MNIST results}
%   \begin{itemize}
%   \item Achieves close to classical CNN accuracy.
%     % \item Pictures to be added.
%     \begin{table}[H]
%       \centering
%       \caption{MNIST performance}
%       \label{t:mn_perf}
%       \begin{tabular}{|l|l|l|l|l|l|l|l|}
%         \hline
%         \multicolumn{2}{|l|}{accuracy} & \multicolumn{2}{l|}{loss} & \multirow{2}{*}{name}       \\ \cline{1-4}
%         test           & train         & test        & train       &                             \\ \hline
%         98.87          & 99.62         & 1.02e+00    & 9.99e-01    & cgconv\_cgconv\_fc\_softmax \\ \hline
%         98.00          & 99.26         & 6.52e-02    & 2.77e-02    & cgconv\_softmax             \\ \hline
%         96.75          & 96.78         & 1.12e+00    & 1.12e+00    & fgconv\_fgconv\_fc\_softmax \\ \hline
%         95.91          & 95.50         & 1.44e-01    & 1.53e-01    & fgconv\_softmax             \\ \hline
%         97.66          & 97.79         & 1.09e+00    & 1.08e+00    & sgconv\_sgconv\_fc\_softmax \\ \hline
%         96.95          & 97.27         & 1.03e-01    & 9.46e-02    & sgconv\_softmax             \\ \hline
%         92.18          & 92.47         & 3.14e-01    & 3.14e-01    & softmax                     \\ \hline
%       \end{tabular}
%     \end{table}
% \end{itemize}
% \end{frame}

% \section{Semi-Supervised Classification with Graph Convolutional Networks}
% \begin{frame}
%   \frametitle{Fast approximate convolutions on the graph}
%   \begin{itemize}
%   \item Layerwise propagation rule as
%     $$H^{(l+1)} = \sigma(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}H^{(l)}W^{(l)})$$
%     Here $\tilde{A} = A + I_N$ the adjacency matrix with added self-loops. $\tilde{D}$ is the degree matrix.
%   \item In the chebyshev approximation, limit to $K=1$ and therefore the layer-wise convolution operation is linear function of the laplacian.
%   \item Experimentally shows 2-3 layered GCN can effectively learn standard graph problems. Specifically it does decently well in the unsupervised case, and significantly good in the semi-supervised setting.
%   \end{itemize}
% \end{frame}

% \section{Limitations of Graph Convolutional Networks}
% \begin{frame}
%   \frametitle{Limitations}
%   \begin{itemize}
%   \item We would like to model a network which doesn't require a rectangular input size and therefore be able to accomodate superpixels. In theory this should be possible, but there are a few points that we need to keep in mind.
%     \begin{itemize}
%     \item Graph doesn't have orientation. There is no sense of up, down, left or right. The filters are rotationally invariant. This can be both advantageous as well as disadvantageous depending on the set-up of the problem. Spatial Transformer Networks learn the invariance to rotation as well as generic warping. But there is always the problem of '6' and '9' because they are equivalent in modulo rotation.
%     \item The filters are not directly transferable to another graph (because of the graph laplacian).
%     \end{itemize}
%   \end{itemize}
% \end{frame}

% \bibliography{ref.bib}
% \bibliographystyle{ieeetr}


\end{document}
